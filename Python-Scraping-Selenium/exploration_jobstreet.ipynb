{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Scraping with Python using Selenium with BeautifulSoup\n",
    "### Target site: jobstreet.co.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm here targeting elements using xPath and Class. Maybe this time it was made with the time you tried this there would be a difference in the name or position in the xPath/Class because I noticed that jobstreet was like using a styled-component, which would randomly generate class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Library\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Selenium Options\n",
    "options = webdriver.ChromeOptions()\n",
    "\n",
    "# Set Selenium to use fullscreen\n",
    "options.add_argument('--start-maximized')\n",
    "\n",
    "# Set Selenium to use Chrome and pass the options\n",
    "driver = webdriver.Chrome('./driver/chromedriver.exe', options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Selenium to open targeted site and wait for 5sec\n",
    "### In this case i set to 5sec for avoid blocking IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.jobstreet.co.id')\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for input elements using xPath and enter the target keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_position_input = driver.find_element_by_xpath('//*[@id=\"searchKeywordsField\"]')\n",
    "job_position_input.send_keys('Data Engineer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for the button and click to find a list of jobs that match the keyword, and of course wait 5sec after that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element_by_xpath('//*[@id=\"contentContainer\"]/div/div[1]/div/div/div/div[2]/div/form/div/div/div[2]/div[4]/button').click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare DataFrame using Pandas Library\n",
    "### In this scraping I want to take some information; Job title, job link, company, company link, location, salary and date of posting of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data = pd.DataFrame({'Link': [],\n",
    "                          'Position': [],\n",
    "                          'Company': [],\n",
    "                          'Company Link': [],\n",
    "                          'Location': [],\n",
    "                          'Salary': [],\n",
    "                          'Published Date': []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the job list page contains several pages, it will be using a While-Loop to reach all available pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial state\n",
    "i = 0\n",
    "\n",
    "# Start scraping\n",
    "while True:\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    # Find the information wrapper (Card)\n",
    "    job_lists = soup.find_all(\n",
    "        'div', class_='sx2jih0 zcydq87a zcydq86a zcydq84y zcydq85a')\n",
    "\n",
    "    # If there is, the initial state will be incremented by one.\n",
    "    i += 1\n",
    "\n",
    "    # Looping the job lists\n",
    "    for job_list in job_lists:\n",
    "        full_link = 'https://www.jobstreet.co.id'\n",
    "        company_temp = job_list.find('a', class_='sx2jih0 sx2jihe _2sRFr')\n",
    "        link = job_list.find(\n",
    "            'a', class_='_18qlyvc12 _9tnmfh1 _18qlyvc2 sx2jih0 sx2jihe zcydq824').get('href')\n",
    "        full_job_link = full_link + link\n",
    "        position = job_list.find('span', class_='sx2jih0').text\n",
    "        company = company_temp.text\n",
    "        company_link = full_link + company_temp.get('href')\n",
    "        location = job_list.find('a', class_='sx2jih0 sx2jihe _2sRFr').text\n",
    "        published_date = job_list.find(\n",
    "            'time', class_='sx2jih0 zcydq82q').attrs['datetime'].strip()\n",
    "\n",
    "        # Because not all salary information is available, try-except is used to fill in the blank salary information.\n",
    "        try:\n",
    "            salary = job_list.find_all(\n",
    "                'span', class_='sx2jih0 zcydq82q')[1].text\n",
    "        except:\n",
    "            salary = 'NA'\n",
    "\n",
    "        # Adding into DataFrame\n",
    "        job_data = job_data.append(\n",
    "            {'Link': full_job_link,\n",
    "             'Position': position,\n",
    "             'Company': company,\n",
    "             'Company Link': company_link,\n",
    "             'Location': location,\n",
    "             'Salary': salary,\n",
    "             'Published Date': published_date}, ignore_index=True)\n",
    "\n",
    "    # Next Page\n",
    "    if i == 1:\n",
    "        try:\n",
    "            next_page = soup.find(\n",
    "                'a', class_='sx2jih0 zcydq872 zcydq862 zcydq88 zcydq82b zcydq832 zcydq8c6 zcydq824 zcydq82l zcydq82k CyicE_0').get('href')\n",
    "            driver.get(full_link + next_page)\n",
    "        except:\n",
    "            break\n",
    "    else:\n",
    "        try:\n",
    "            next_page = soup.find_all(\n",
    "                'a', class_='sx2jih0 zcydq872 zcydq862 zcydq88 zcydq82b zcydq832 zcydq8c6 zcydq824 zcydq82l zcydq82k CyicE_0')[1].get('href')\n",
    "            driver.get(full_link + next_page)\n",
    "        except:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export DataFrame to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.to_csv('job_lists.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
